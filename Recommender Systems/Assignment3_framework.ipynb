{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load necesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1      186      302       3  891717742\n",
       "3      244       51       2  880606923\n",
       "5      298      474       4  884182806\n",
       "6      115      265       2  881171488\n",
       "7      253      465       5  891628467"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "df = pd.read_csv('ml-100k/u.data', names=['user_id', 'item_id', 'rating', 'timestamp'], sep='\\t')\n",
    "\n",
    "# obtain top 500 users and top 500 items\n",
    "user_ids = df.groupby('user_id').count().sort_values(by='rating', ascending=False).head(500).index\n",
    "item_ids = df.groupby('item_id').count().sort_values(by='rating', ascending=False).head(500).index\n",
    "df = df[(df['user_id'].isin(user_ids)) & (df['item_id'].isin(item_ids))]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select one rating from each user as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "# remap user and item ID\n",
    "df['user_id'] = df.groupby('user_id').ngroup()\n",
    "df['item_id'] = df.groupby('item_id').ngroup()\n",
    "\n",
    "test_df = df.groupby('user_id').sample(1, random_state=1024)\n",
    "train_df = df[~df.index.isin(test_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of users: 500\n",
      "The number of items: 500\n",
      "Avg. # of rated Items/User: 129.914\n",
      "Density of data: 0.259828\n",
      "Ratings Range: 1 - 5\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "avg_num = df.groupby('user_id').size().mean()\n",
    "density = df.shape[0] / (n_users * n_items)\n",
    "min_ratings = df.rating.min()\n",
    "max_ratings = df.rating.max()\n",
    "\n",
    "print(\"The number of users: {}\" .format(n_users))\n",
    "print(\"The number of items: {}\" .format(n_items))\n",
    "print(\"Avg. # of rated Items/User: {}\" .format(avg_num))\n",
    "print(\"Density of data: {}\" .format(density))\n",
    "print(\"Ratings Range: {} - {}\" .format(min_ratings, max_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct the rating matrix based on train_df:\n",
      "[[5. 3. 4. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " [4. 3. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 4. 0.]]\n",
      "Construct the rating matrix based on test_df:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "# Convert the format of datasets to matrices\n",
    "# Train dataset\n",
    "df_zeros = pd.DataFrame({\n",
    "    'user_id': np.tile(np.arange(0, n_users), n_items), \n",
    "    'item_id': np.repeat(np.arange(0, n_items), n_users), \n",
    "    'rating': 0})\n",
    "train_ds = df_zeros.merge(train_df, \n",
    "                          how='left', \n",
    "                          on=['user_id', 'item_id']).fillna(0.).pivot_table(\n",
    "                              values='rating_y', \n",
    "                              index='user_id', \n",
    "                              columns='item_id').values\n",
    "                           \n",
    "# Test dataset\n",
    "test_ds = df_zeros.merge(test_df, \n",
    "                         how='left', \n",
    "                         on=['user_id', 'item_id']).fillna(0.).pivot_table(\n",
    "                             values='rating_y', \n",
    "                             index='user_id', \n",
    "                             columns='item_id').values\n",
    "\n",
    "print(\"Construct the rating matrix based on train_df:\")\n",
    "print(train_ds)\n",
    "\n",
    "print(\"Construct the rating matrix based on test_df:\")\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def user_corr(imputed_train_ds):\n",
    "    '''\n",
    "    Function for calculating user's similarity\n",
    "    '''\n",
    "    active_user_pearson_corr = np.zeros((imputed_train_ds.shape[0], imputed_train_ds.shape[0]))\n",
    "\n",
    "    # Compute Pearson Correlation Coefficient of All Pairs of Users between active set and training dataset\n",
    "    for i, user_i_vec in enumerate(imputed_train_ds):\n",
    "        for j, user_j_vec in enumerate(imputed_train_ds):\n",
    "\n",
    "            # ratings corated by the current pair od users\n",
    "            mask_i = user_i_vec > 0\n",
    "            mask_j = user_j_vec > 0\n",
    "\n",
    "            # corrated item index, skip if there are no corrated ratings\n",
    "            corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "            if len(corrated_index) == 0:\n",
    "                continue\n",
    "\n",
    "            # average value of user_i_vec and user_j_vec\n",
    "            mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "            mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "            # compute pearson corr\n",
    "            user_i_sub_mean = user_i_vec[corrated_index] - mean_user_i\n",
    "            user_j_sub_mean = user_j_vec[corrated_index] - mean_user_j\n",
    "\n",
    "            r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "            r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "            r_ui_sum_sqrt = np.sqrt(np.sum(r_ui_sub_r_i_sq))\n",
    "            r_uj_sum_sqrt = np.sqrt(np.sum(r_uj_sub_r_j_sq))\n",
    "\n",
    "            sim = np.sum(user_i_sub_mean * user_j_sub_mean) / (r_ui_sum_sqrt * r_uj_sum_sqrt + EPSILON)\n",
    "            active_user_pearson_corr[i][j] = sim\n",
    "\n",
    "    return active_user_pearson_corr\n",
    "\n",
    "def predict(test_ds, imputed_train_ds, user_corr, k=20):\n",
    "    '''\n",
    "    Function for predicting ratings in test_ds\n",
    "    '''\n",
    "\n",
    "    # Predicting ratings of test set\n",
    "    predicted_ds = np.zeros_like(test_ds)\n",
    "\n",
    "    for (i, j), rating in np.ndenumerate(test_ds):\n",
    "\n",
    "        if rating > 0:\n",
    "\n",
    "            # only predict ratings on test set\n",
    "            sim_user_ids = np.argsort(user_corr[i])[-1:-(k + 1):-1]\n",
    "\n",
    "            #==================user-based==================#\n",
    "            # the coefficient values of similar users\n",
    "            sim_val = user_corr[i][sim_user_ids]\n",
    "\n",
    "            # the average value of the current user's ratings\n",
    "            sim_users = imputed_train_ds[sim_user_ids]\n",
    "            \n",
    "            mask_rateditem_user = imputed_train_ds[i] != 0\n",
    "            num_rated_items = mask_rateditem_user.astype(np.float32)\n",
    "            user_mean = np.sum(imputed_train_ds[i, mask_rateditem_user]) / (num_rated_items.sum() + EPSILON)\n",
    "\n",
    "            mask_nei_rated_items = sim_users != 0\n",
    "            num_rated_per_user = mask_nei_rated_items.astype(np.float32)\n",
    "            num_per_user = num_rated_per_user.sum(axis=1)\n",
    "\n",
    "            sum_per_user = sim_users.sum(axis=1)\n",
    "            sim_user_mean = sum_per_user / (num_per_user + EPSILON)\n",
    "            \n",
    "            mask_rated_j = sim_users[:, j] > 0\n",
    "                            \n",
    "            # sim(u, v) * (r_vj - mean_v)\n",
    "            sim_r_sum_mean = sim_val[mask_rated_j] * (sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])\n",
    "            \n",
    "            user_based_pred = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "\n",
    "            predicted_ds[i, j] = np.clip(user_based_pred, 0, 5)\n",
    "            \n",
    "    return predicted_ds\n",
    "\n",
    "def evaluate(test_ds, predicted_ds):\n",
    "    '''\n",
    "    Function for evaluating on MAE and RMSE\n",
    "    '''\n",
    "    # MAE\n",
    "    mask_test_ds = test_ds > 0\n",
    "    MAE = np.sum(np.abs(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32))\n",
    "\n",
    "    # RMSE\n",
    "    RMSE = np.sqrt(np.sum(np.square(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32)))\n",
    "\n",
    "    return MAE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - KNN based recommendation (Similarity Metric: Pearson Correlation Coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "user_pearson_corr = user_corr(train_ds)\n",
    "predicted_ds = predict(test_ds, train_ds, user_pearson_corr, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Baseline Result =====================\n",
      "MAE: 0.8471711011333851, RMSE: 1.092846045041526\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "MAE, RMSE = evaluate(test_ds, predicted_ds)\n",
    "\n",
    "print(\"===================== Baseline Result =====================\")\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Solution\n",
    "(Put all your implementation for your solution in the following cell only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly, creating a new dataframe which consists of the train_ds and test_ds- the sets I have to work on. \n",
    "\n",
    "train_ds =pd.DataFrame (train_ds)\n",
    "test_ds =pd.DataFrame (test_ds)\n",
    "\n",
    "#First the important step in the implementation is to calculate the P(t) value- which will give us the rating$ of what \n",
    "# every user gave for item t \n",
    "\n",
    "# I create a list for P(t)\n",
    "# POPULARITY = P(t)\n",
    "item_t_pop = []\n",
    "\n",
    "for i in train_ds.columns:\n",
    "    pop= train_ds[i].value_counts().sum()\n",
    "    item_t_pop.append(pop)\n",
    "    \n",
    "# A very important and new implementation step is to compute the weight \n",
    "\n",
    "# the weight supports that the less popular items contribute to a higher significance\n",
    "# n_users= 500 is the total number of users in database and i is the item_t_popularity\n",
    "# W(t) = weight\n",
    "weight_item_t= []\n",
    "for i in item_t_pop:\n",
    "    weight= np.log (n_users/i)\n",
    "    weight_item_t.append(weight) \n",
    "    \n",
    "    \n",
    "# Using epsilon to avoid errors and gamma =30 as based on research it is a good number,\n",
    "# this is even more reliable as if lesser than this it will reduce its significance.\n",
    "\n",
    "EPSILON = 1e-9\n",
    "GAMMA = 30\n",
    "\n",
    "pearson_correlation = np.zeros((n_users, n_users))\n",
    "\n",
    "for i, user_a_v in enumerate(train_ds.values):\n",
    "    for j, user_u_v in enumerate(train_ds.values):\n",
    "\n",
    "        # calculating the ratings of the pairs of users who corated \n",
    "        user_a_val = user_a_v > 0\n",
    "        user_u_val = user_u_v > 0\n",
    "\n",
    "        # calculating the union for user a and u\n",
    "        # the if condition is used when the corrated items are larger than 0 then only I can calculate \n",
    "        # the similarity, if it is lower than 0 those ratings should not be taken forward as no similarity \n",
    "        # between the two\n",
    "        items_union_a_u= np.union1d(np.where(user_a_val),np.where(user_u_val))\n",
    "        if len (items_union_a_u) == 0: \n",
    "            continue\n",
    "            \n",
    "        # calculating the mean for users a and u\n",
    "        # using np.clip to keep all values below 0 as o and above 1 as 1\n",
    "        mean_user_a = np.sum(user_a_v) / (np.sum(np.clip(user_a_v, 0, 1)) + EPSILON)\n",
    "        mean_user_u = np.sum(user_u_v) / (np.sum(np.clip(user_u_v, 0, 1)) + EPSILON)\n",
    "        \n",
    "        # adding missing data\n",
    "        # here for adding the missing data created a for loop in which finding the item set the user has not rated\n",
    "        # and making it == the mean for that user\n",
    "        for ind in train_ds.index:\n",
    "            if(ind == i): \n",
    "                for index in items_union_a_u:\n",
    "                    if(train_ds[index][ind] == 0) :\n",
    "                        train_ds[index][ind] = mean_user_a\n",
    "        \n",
    "        for ind in train_ds.index:\n",
    "            if(ind == j): \n",
    "                for index in items_union_a_u:\n",
    "                    if(train_ds[index][ind] == 0) :\n",
    "                        train_ds[index][ind] = mean_user_u\n",
    "                    \n",
    "        # Introducing the weight element here to find the new pcc\n",
    "        # add each item's weight\n",
    "        weight_square_a_u = []\n",
    "        for i in items_union_a_u:\n",
    "            weight_square_a_u.append(weight_item_t[i]* weight_item_t[i])\n",
    "        weight_square_a_u = pd.Series(weight_square_a_u) \n",
    "        # Changed the weight square into a series as in an array an error will show as it will not be able to \n",
    "        #compute the values. Therefore, converted it into series.\n",
    "\n",
    "        #Calculating the user a and u sub mean\n",
    "        user_a_sub_mean = user_a_v[items_union_a_u] - mean_user_a\n",
    "        user_u_sub_mean = user_u_v[items_union_a_u] - mean_user_u\n",
    "\n",
    "        #squaring the means\n",
    "        sq_a_mean = np.square(user_a_sub_mean)\n",
    "        sq_u_mean = np.square(user_u_sub_mean)\n",
    "        \n",
    "        #denominator calculation for PCC\n",
    "        # multiplying the weight square\n",
    "        a_sqrt = np.sqrt(np.sum(weight_square_a_u*sq_a_mean))\n",
    "        u_sqrt = np.sqrt(np.sum(weight_square_a_u*sq_u_mean))\n",
    "\n",
    "        #numerator calculation for PCC\n",
    "        new_user_au_sub_mean= weight_square_a_u*user_a_sub_mean*user_u_sub_mean\n",
    "        sim_au = np.sum(new_user_au_sub_mean) / (a_sqrt * u_sqrt + EPSILON)\n",
    "        \n",
    "        #calculating the significance weighting\n",
    "        sig_wt = (min(len(items_union_a_u), GAMMA) / GAMMA) * sim_au\n",
    "\n",
    "        pearson_correlation[i][j] = sig_wt \n",
    "\n",
    "\n",
    "#after finding the similarities between pairs I can move on to prediction\n",
    "\n",
    "#Predict the ratings\n",
    "\n",
    "prediction_matrix = np.zeros((n_users, n_items))\n",
    "\n",
    "K = 20\n",
    "EPSILON = 1e-9\n",
    "\n",
    "# I use denumerate here as I want to do the prediction for every rating in the test set by checking the values \n",
    "# in a and u\n",
    "# here i is the active user and it can be anyone in the test set, and t is the item that user is rating\n",
    "\n",
    "for (i, t), rating in np.ndenumerate(test_ds.values):\n",
    "    if rating > 0:\n",
    "        # i is the active user\n",
    "        #argsort is used to sort all the current users based on similarity in an ascending order\n",
    "        # and -1 is used here as I need to access from the bottom i.e., the last value. Because argsort sorts it\n",
    "        #in a descending order. \n",
    "        \n",
    "        similar_users = np.argsort(pearson_correlation[i])[-(K + 1):-1]\n",
    "\n",
    "        # similarity values\n",
    "        coeff_val = pearson_correlation[i][similar_users]\n",
    "\n",
    "        # value of similar users.'similar_users' from the k nearest neighbours\n",
    "        #this matrix is smaller which includes all the k-nearest neighbours from all rows\n",
    "        #each value is taken from the train_ds\n",
    "        k_sim_users = train_ds.values[similar_users]\n",
    "        \n",
    "        #mean values for the current user\n",
    "        #this is a calculation just for one row [i]\n",
    "        current_user_mean = np.sum(train_ds.values[i]) / (np.sum(np.clip(train_ds.values[i], 0, 1)) + EPSILON)\n",
    "        \n",
    "        # this gives mean values for all selected k neighbours \n",
    "        # while this calculates for k rows for the k -nearest neighbours\n",
    "        #axis=1 is for every row\n",
    "        k_neighbours_mean = np.sum(k_sim_users, axis=1) / (np.sum(np.clip(k_sim_users, 0, 1), axis=1) + EPSILON)\n",
    "\n",
    "        # all the users in k-nearest neighbours who rated item t\n",
    "        rating_t = k_sim_users[:, t] > 0\n",
    "        \n",
    "        final_value = coeff_val[rating_t] * (k_sim_users[rating_t, t] - k_neighbours_mean[rating_t])\n",
    "\n",
    "        \n",
    "        prediction_matrix[i][t] = current_user_mean + np.sum(final_value) / (np.sum(coeff_val[rating_t]) + EPSILON)\n",
    "        # using a clip, just incase to change the rating values which are out of range to within the range of 0 to 5\n",
    "        # as mentioned in the readme file for the given dataset\n",
    "        \n",
    "        prediction_matrix[i][t] = np.clip(prediction_matrix[i][t], 0, 5)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ##Finally, calculating the MAE and RMSE values   \n",
    "\n",
    "#==================MAE on Testing set===================#\n",
    "actual_ratings = test_ds.values\n",
    "\n",
    "# calculating the differences and absolute errors\n",
    "error_abs = np.abs(prediction_matrix - actual_ratings)\n",
    "\n",
    "# calculating all the ratings in the test set \n",
    "# this is the weight\n",
    "wt = np.clip(actual_ratings, 0, 1)\n",
    "\n",
    "# calculating the absoulte error only on the rated ratings\n",
    "error_abs_rated = error_abs * wt\n",
    "\n",
    "# MAE\n",
    "MAE = np.sum(error_abs_rated) / np.sum(wt) \n",
    "\n",
    "#==================RMSE on Testing set===================#\n",
    "actual_ratings = test_ds.values\n",
    "\n",
    "# calculating the squared error\n",
    "sqr_error = np.square(prediction_matrix - actual_ratings)\n",
    "wt = np.clip(actual_ratings, 0, 1)\n",
    "\n",
    "# calculating the squared error only on the rated ratings\n",
    "sqr_error = sqr_error * wt\n",
    "\n",
    "# RMSE\n",
    "RMSE = np.sqrt(np.sum(sqr_error) / np.sum(wt))\n",
    "\n",
    "#RMSE tries to panelize the larger errors more hence, it will always be greater than the MAE\n",
    "\n",
    "\n",
    "\n",
    "# RECOMMENDATION- ComRV calculation\n",
    "#ComRV= P(t)*R(t)[mean]\n",
    "\n",
    "sum_ratings  = train_ds.sum(axis=0)\n",
    "\n",
    "Popularity = pd.Series(item_t_pop)\n",
    "\n",
    "#to find out R(T)[mean], calculting the sum of ratings divided by the item popularity P(t)\n",
    "mean = sum_ratings/Popularity\n",
    "\n",
    "#using np.clip to keep it within range\n",
    "mean = np.clip(mean, 0, 5)\n",
    "\n",
    "#finally, multiply the Popularity with the mean value R(t)\n",
    "Com_RV = Popularity*mean\n",
    "\n",
    "# calculating the top n highest prediction value- the top 10 \n",
    "\n",
    "Top_n_most_pop= Com_RV.nlargest(n=10, keep='first').sort_values(ascending=False)\n",
    "\n",
    "MAE = 0.8381396662804781 \n",
    "RMSE = 1.0384051910547503\n",
    "\n",
    "# In my implementation as you can notice, my MAE and RMSE values are lower than the baseline recommendation which is\n",
    "# MAE= 0.8471711011333851, RMSE: 1.092846045041526"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the MAE and RMSE of Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== The MAE and RMSE of Your Implementation =====================\n",
      "MAE: 0.8381396662804781, RMSE: 1.0384051910547503\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "print(\"===================== The MAE and RMSE of Your Implementation =====================\")\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
